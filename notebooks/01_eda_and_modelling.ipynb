{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datium Data Science Assessment – Section A\n",
    "**Goal:** Predict vehicle sale prices (`Sold_Amount`).\n",
    "\n",
    "This notebook follows a structured ML workflow:\n",
    "1. Data loading & initial inspection\n",
    "2. Exploratory Data Analysis (EDA)\n",
    "3. Feature engineering & encoding\n",
    "4. Model experimentation with MLflow tracking\n",
    "5. Model evaluation & diagnostics\n",
    "6. Final model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.insert(0, os.path.join(os.getcwd(), '..', 'src'))\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import mlflow\n",
    "import shap\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "from trainer import VehiclePriceTrainer, BANNED_FEATURES, TARGET\n",
    "\n",
    "pd.set_option('display.max_columns', 60)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "sns.set_theme(style='whitegrid', palette='muted')\n",
    "\n",
    "SEED = 42\n",
    "DATA_DIR = '../data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_raw = pd.read_csv(f'{DATA_DIR}/train.rpt', sep='\\t', low_memory=False, encoding='utf-8-sig')\n",
    "test_raw  = pd.read_csv(f'{DATA_DIR}/test.rpt',  sep='\\t', low_memory=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f'Train shape: {train_raw.shape}')\n",
    "print(f'Test  shape: {test_raw.shape}')\n",
    "train_raw.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Understanding & Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- missing values ----\n",
    "missing = train_raw.isnull().mean().sort_values(ascending=False)\n",
    "missing_pct = (missing * 100).round(1)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "missing_pct[missing_pct > 0].head(40).plot(kind='barh', ax=ax)\n",
    "ax.set_title('Missing value rate by feature (train, top 40)')\n",
    "ax.set_xlabel('% missing')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../logs/missing_values.png', dpi=120)\n",
    "plt.show()\n",
    "\n",
    "print(f'Columns with >50% missing: {(missing_pct > 50).sum()}')\n",
    "print(f'Columns with >90% missing: {(missing_pct > 90).sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- target distribution ----\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "train_raw[TARGET].dropna().plot(kind='hist', bins=60, ax=axes[0])\n",
    "axes[0].set_title('Sold_Amount distribution')\n",
    "axes[0].set_xlabel('Price ($)')\n",
    "\n",
    "np.log1p(train_raw[TARGET].dropna()).plot(kind='hist', bins=60, ax=axes[1], color='steelblue')\n",
    "axes[1].set_title('log1p(Sold_Amount) distribution')\n",
    "axes[1].set_xlabel('log1p(Price)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../logs/target_distribution.png', dpi=120)\n",
    "plt.show()\n",
    "\n",
    "print(train_raw[TARGET].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- outlier flag ----\n",
    "q1, q99 = train_raw[TARGET].quantile([0.01, 0.99])\n",
    "outliers = ((train_raw[TARGET] < q1) | (train_raw[TARGET] > q99)).sum()\n",
    "print(f'Rows outside 1st–99th percentile: {outliers} ({outliers/len(train_raw)*100:.1f}%)')\n",
    "print(f'1st pct: ${q1:,.0f}   99th pct: ${q99:,.0f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- duplicate check ----\n",
    "print(f'Duplicate rows: {train_raw.duplicated().sum()}')\n",
    "\n",
    "# ---- NULL string check (common in .rpt exports) ----\n",
    "null_str_cols = [c for c in train_raw.columns if (train_raw[c] == 'NULL').any()]\n",
    "print(f'Columns containing literal \"NULL\" string: {len(null_str_cols)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df: pd.DataFrame, is_train: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"Clean and engineer features.\"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # Replace literal NULL strings with NaN\n",
    "    df.replace('NULL', np.nan, inplace=True)\n",
    "\n",
    "    # ---------- drop banned columns ----------\n",
    "    df.drop(columns=[c for c in BANNED_FEATURES if c in df.columns], inplace=True)\n",
    "\n",
    "    # ---------- drop columns with >70% missing ----------\n",
    "    if is_train:\n",
    "        global HIGH_MISS_COLS\n",
    "        HIGH_MISS_COLS = df.columns[df.isnull().mean() > 0.70].tolist()\n",
    "    df.drop(columns=[c for c in HIGH_MISS_COLS if c in df.columns], inplace=True)\n",
    "\n",
    "    # ---------- date features ----------\n",
    "    for col in ['Sold_Date', 'Compliance_Date']:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "\n",
    "    if 'Sold_Date' in df.columns:\n",
    "        df['Sold_Year']  = df['Sold_Date'].dt.year\n",
    "        df['Sold_Month'] = df['Sold_Date'].dt.month\n",
    "        df['Sold_Quarter'] = df['Sold_Date'].dt.quarter\n",
    "\n",
    "    # ---------- vehicle age at sale ----------\n",
    "    if 'YearGroup' in df.columns and 'Sold_Date' in df.columns:\n",
    "        df['Age_At_Sale'] = df['Sold_Date'].dt.year - pd.to_numeric(df['YearGroup'], errors='coerce')\n",
    "\n",
    "    # ---------- KM per year ----------\n",
    "    if 'KM' in df.columns and 'Age_Comp_Months' in df.columns:\n",
    "        df['KM'] = pd.to_numeric(df['KM'], errors='coerce')\n",
    "        df['Age_Comp_Months'] = pd.to_numeric(df['Age_Comp_Months'], errors='coerce')\n",
    "        df['KM_Per_Month'] = df['KM'] / (df['Age_Comp_Months'].replace(0, np.nan))\n",
    "\n",
    "    # ---------- engine size numeric ----------\n",
    "    if 'EngineSize' in df.columns:\n",
    "        df['EngineSize'] = pd.to_numeric(df['EngineSize'], errors='coerce')\n",
    "\n",
    "    # ---------- NewPrice log ----------\n",
    "    if 'NewPrice' in df.columns:\n",
    "        df['NewPrice'] = pd.to_numeric(df['NewPrice'], errors='coerce')\n",
    "        df['NewPrice_log'] = np.log1p(df['NewPrice'])\n",
    "        df['Depreciation_Pct'] = 1 - (df['Sold_Amount'] / df['NewPrice'].replace(0, np.nan)) if is_train else np.nan\n",
    "\n",
    "    # drop raw date columns\n",
    "    df.drop(columns=['Sold_Date', 'Compliance_Date'], errors='ignore', inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "HIGH_MISS_COLS = []  # will be populated during train preprocessing\n",
    "train_proc = preprocess(train_raw, is_train=True)\n",
    "test_proc  = preprocess(test_raw,  is_train=False)\n",
    "\n",
    "print(f'Processed train shape: {train_proc.shape}')\n",
    "print(f'Processed test  shape: {test_proc.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- feature/target split ----\n",
    "# Drop rows with missing target\n",
    "train_clean = train_proc.dropna(subset=[TARGET]).copy()\n",
    "\n",
    "# Use log-target for modelling (reduces skew impact)\n",
    "y_log = np.log1p(train_clean[TARGET])\n",
    "\n",
    "# Remove target + leakage columns from features\n",
    "drop_for_features = [TARGET, 'Depreciation_Pct']\n",
    "X = train_clean.drop(columns=[c for c in drop_for_features if c in train_clean.columns])\n",
    "\n",
    "# ---- identify column types ----\n",
    "num_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "cat_cols = X.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "\n",
    "# Limit cardinality for ordinal encoding\n",
    "high_card = [c for c in cat_cols if X[c].nunique() > 200]\n",
    "cat_cols  = [c for c in cat_cols if c not in high_card]\n",
    "X = X.drop(columns=high_card)\n",
    "\n",
    "print(f'Numeric features : {len(num_cols)}')\n",
    "print(f'Categorical features: {len(cat_cols)}')\n",
    "print(f'Dropped high-cardinality: {high_card}')\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y_log, test_size=0.2, random_state=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler',  StandardScaler()),\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)),\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', numeric_transformer, num_cols),\n",
    "    ('cat', categorical_transformer, cat_cols),\n",
    "], remainder='drop')\n",
    "\n",
    "print('Preprocessor ready.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Experiments with MLflow Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri('../logs/mlruns')\n",
    "\n",
    "MODELS = {\n",
    "    'Ridge': Ridge(alpha=10.0),\n",
    "    'RandomForest': RandomForestRegressor(n_estimators=200, max_depth=15, n_jobs=-1, random_state=SEED),\n",
    "    'XGBoost': XGBRegressor(\n",
    "        n_estimators=500, learning_rate=0.05, max_depth=6,\n",
    "        subsample=0.8, colsample_bytree=0.8,\n",
    "        n_jobs=-1, random_state=SEED, verbosity=0\n",
    "    ),\n",
    "    'LightGBM': LGBMRegressor(\n",
    "        n_estimators=500, learning_rate=0.05, num_leaves=63,\n",
    "        subsample=0.8, colsample_bytree=0.8,\n",
    "        n_jobs=-1, random_state=SEED, verbose=-1\n",
    "    ),\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, estimator in MODELS.items():\n",
    "    print(f'\\n===== {name} =====')\n",
    "    pipe = Pipeline([('pre', preprocessor), ('model', estimator)])\n",
    "    trainer = VehiclePriceTrainer(\n",
    "        pipeline=pipe,\n",
    "        experiment_name='vehicle_price',\n",
    "        cv_folds=5,\n",
    "        models_dir='../models',\n",
    "    )\n",
    "    trainer.fit(\n",
    "        X_train, y_train,\n",
    "        X_val=X_val, y_val=y_val,\n",
    "        run_name=name,\n",
    "        extra_tags={'model_type': name},\n",
    "    )\n",
    "    val_metrics = trainer.evaluate(X_val, y_val)\n",
    "    results[name] = val_metrics\n",
    "\n",
    "results_df = pd.DataFrame(results).T\n",
    "print('\\n--- Validation Results Summary ---')\n",
    "print(results_df.sort_values('MAE'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use best model (lowest MAE) for diagnostics\n",
    "best_name = results_df['MAE'].idxmin()\n",
    "print(f'Best model: {best_name}')\n",
    "\n",
    "best_pipe = Pipeline([('pre', preprocessor), ('model', MODELS[best_name])])\n",
    "best_pipe.fit(X_train, y_train)\n",
    "\n",
    "val_pred_log = best_pipe.predict(X_val)\n",
    "val_pred     = np.expm1(val_pred_log)   # back to dollars\n",
    "val_true     = np.expm1(y_val.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# -- Actual vs Predicted --\n",
    "axes[0].scatter(val_true, val_pred, alpha=0.3, s=8)\n",
    "mn, mx = val_true.min(), val_true.max()\n",
    "axes[0].plot([mn, mx], [mn, mx], 'r--')\n",
    "axes[0].set_xlabel('Actual ($)')\n",
    "axes[0].set_ylabel('Predicted ($)')\n",
    "axes[0].set_title('Actual vs Predicted')\n",
    "\n",
    "# -- Residuals --\n",
    "residuals = val_true - val_pred\n",
    "axes[1].scatter(val_pred, residuals, alpha=0.3, s=8)\n",
    "axes[1].axhline(0, color='r', linestyle='--')\n",
    "axes[1].set_xlabel('Predicted ($)')\n",
    "axes[1].set_ylabel('Residual ($)')\n",
    "axes[1].set_title('Residuals vs Predicted')\n",
    "\n",
    "# -- Residual distribution --\n",
    "axes[2].hist(residuals, bins=60, edgecolor='k', alpha=0.7)\n",
    "axes[2].set_xlabel('Residual ($)')\n",
    "axes[2].set_title('Residual Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../logs/diagnostics.png', dpi=120)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Error by price bucket ----\n",
    "buckets = pd.cut(val_true, bins=[0, 5000, 10000, 20000, 40000, 80000, np.inf],\n",
    "                 labels=['<5k','5-10k','10-20k','20-40k','40-80k','>80k'])\n",
    "bucket_df = pd.DataFrame({'true': val_true, 'pred': val_pred, 'bucket': buckets})\n",
    "bucket_df['APE'] = np.abs(bucket_df['true'] - bucket_df['pred']) / bucket_df['true'].replace(0, np.nan)\n",
    "\n",
    "bucket_summary = bucket_df.groupby('bucket', observed=True)['APE'].agg(['mean','count'])\n",
    "bucket_summary.columns = ['MAPE', 'Count']\n",
    "bucket_summary['MAPE'] = (bucket_summary['MAPE'] * 100).round(1)\n",
    "\n",
    "print('Error by price bucket:')\n",
    "print(bucket_summary)\n",
    "\n",
    "bucket_summary['MAPE'].plot(kind='bar', figsize=(8,4), title='MAPE by Price Bucket (%)', rot=0)\n",
    "plt.ylabel('MAPE (%)')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../logs/error_by_bucket.png', dpi=120)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Feature importance (SHAP) ----\n",
    "X_val_transformed = best_pipe.named_steps['pre'].transform(X_val)\n",
    "feature_names = num_cols + cat_cols\n",
    "\n",
    "explainer = shap.TreeExplainer(best_pipe.named_steps['model'])\n",
    "shap_values = explainer.shap_values(X_val_transformed)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "shap.summary_plot(shap_values, X_val_transformed, feature_names=feature_names, show=False, max_display=20)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../logs/shap_summary.png', dpi=120)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Final Predictions on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Align test columns to training columns\n",
    "X_test = test_proc.reindex(columns=X.columns)\n",
    "\n",
    "# Fit final model on all training data\n",
    "final_pipe = Pipeline([('pre', preprocessor), ('model', MODELS[best_name])])\n",
    "\n",
    "X_all = pd.concat([X_train, X_val])\n",
    "y_all = pd.concat([y_train, y_val])\n",
    "final_pipe.fit(X_all, y_all)\n",
    "\n",
    "test_pred_log = final_pipe.predict(X_test)\n",
    "test_predictions = np.expm1(test_pred_log)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    'Predicted_Sold_Amount': test_predictions.round(2)\n",
    "})\n",
    "submission.to_csv('../logs/predictions.csv', index=True)\n",
    "print(f'Predictions saved. Shape: {submission.shape}')\n",
    "print(submission['Predicted_Sold_Amount'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "| Step | Description |\n",
    "|------|-------------|\n",
    "| Data understanding | Checked missing rates, target distribution, outliers, duplicate rows, literal NULL strings |\n",
    "| Feature engineering | Date decomposition, age at sale, KM/month, log-NewPrice, depreciation pct |\n",
    "| Encoding | OrdinalEncoder for categoricals; median imputation for numerics |\n",
    "| Models | Ridge, RandomForest, XGBoost, LightGBM |\n",
    "| Evaluation | 5-fold CV (MAE/RMSE/R²), hold-out validation, residual plots, SHAP |\n",
    "| Tracking | MLflow (metrics, params, artefacts) |\n",
    "| Diagnostics | Residual patterns, error-by-price-bucket analysis |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
