{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datium Data Science Assessment – Section B\n",
    "## Visual Analysis of Vehicle Images\n",
    "\n",
    "This notebook is a **design and strategy report** — not a training run (no labelled image dataset is provided).  \n",
    "The goal is to demonstrate how a practical end-to-end image-intelligence system would be designed for an automotive assessment business.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Problem Framing\n",
    "\n",
    "### Business Context\n",
    "Datium receives **user-submitted vehicle images** as part of commercial assessments (e.g. trade-in, insurance, fleet valuation).  \n",
    "The images are uncontrolled: variable lighting, angles, backgrounds, and image quality.\n",
    "\n",
    "### Possible Actionable Outputs\n",
    "\n",
    "| Signal | Value to Business |\n",
    "|--------|------------------|\n",
    "| **Damage detection** | Identify dents, scratches, cracks → adjust valuation downward |\n",
    "| **Vehicle condition score** | Aggregate visual health score (0–10) |\n",
    "| **Colour identification** | Automate a common data entry field |\n",
    "| **Body style classification** | Sedan vs SUV vs Ute → supports market segmentation |\n",
    "| **Odometer/dash reading** | OCR from interior photo → cross-validate KM field |\n",
    "| **Image quality gating** | Reject blurry/incomplete images before downstream processing |\n",
    "\n",
    "### Chosen Focus: Damage Detection + Condition Scoring\n",
    "This is the highest-value signal: damage directly affects resale price, it is labour-intensive to assess manually, and modern vision models are well-suited to it.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Assumptions & Challenges\n",
    "\n",
    "### What we assume about incoming images\n",
    "- JPEG/PNG, typical smartphone quality (2–12 MP)\n",
    "- 1–10 photos per vehicle (exterior: front, rear, sides; interior; odometer)\n",
    "- Submitted by non-expert users — inconsistent framing, glare, obstructions\n",
    "\n",
    "### Key data challenges\n",
    "\n",
    "| Challenge | Mitigation |\n",
    "|-----------|------------|\n",
    "| **No labelled damage dataset** | Combine public datasets (CARDD, VCoR) with in-house labelling (Label Studio) |\n",
    "| **Class imbalance** (most cars undamaged) | Focal loss, oversampling damaged examples, threshold tuning |\n",
    "| **Variable image angle** | Multi-view aggregation; angle classification as pre-step |\n",
    "| **Low-quality images** | Image quality classifier as first gate; feedback to user |\n",
    "| **Privacy (licence plates, faces)** | Auto-blur in pre-processing pipeline |\n",
    "| **Adversarial submissions** (hiding damage) | Anomaly detection; consistency checks against odometer KM |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Modelling Approaches\n",
    "\n",
    "### 3.1 Approach A – Fine-tuned CNN Classifier (Baseline)\n",
    "\n",
    "```\n",
    "Input image → EfficientNet-B3 backbone (ImageNet pretrained)\n",
    "           → Global average pooling\n",
    "           → Dense head → Condition score (regression) + Damage present (binary)\n",
    "```\n",
    "\n",
    "- **Pros:** Fast inference, well-understood, easy to deploy\n",
    "- **Cons:** No spatial localisation — can't say *where* the damage is\n",
    "\n",
    "---\n",
    "\n",
    "### 3.2 Approach B – Object Detection for Damage Localisation\n",
    "\n",
    "```\n",
    "Input image → YOLOv8 / RT-DETR\n",
    "           → Bounding boxes: {scratch, dent, crack, rust, broken_glass}\n",
    "           → Damage area fraction → condition score formula\n",
    "```\n",
    "\n",
    "- **Pros:** Interpretable (show bounding boxes in app), actionable per-damage-type pricing\n",
    "- **Cons:** Requires polygon/bbox annotations (expensive)\n",
    "\n",
    "---\n",
    "\n",
    "### 3.3 Approach C – Vision–Language Foundation Model (Zero/Few-Shot)\n",
    "\n",
    "```\n",
    "Input image + prompt → GPT-4o / Claude 3.5 Sonnet vision\n",
    "                     → Structured JSON: {has_damage, severity, affected_panels, notes}\n",
    "```\n",
    "\n",
    "Prompt example:\n",
    "```\n",
    "\"Inspect this vehicle image. Return JSON:\n",
    "  has_damage (bool), severity (none/minor/moderate/severe),\n",
    "  affected_panels (list), confidence (0–1).\n",
    "  Focus only on visible damage to paintwork, glass, and body panels.\"\n",
    "```\n",
    "\n",
    "- **Pros:** No training data needed to start; handles edge cases well; human-readable explanations\n",
    "- **Cons:** API cost at scale; latency; not suitable for offline/edge deployment\n",
    "\n",
    "---\n",
    "\n",
    "### 3.4 Recommended Hybrid Architecture\n",
    "\n",
    "```\n",
    "                    ┌─────────────────────────────────┐\n",
    "                    │      Image Ingestion API         │\n",
    "                    └──────────────┬──────────────────┘\n",
    "                                   │\n",
    "                    ┌──────────────▼──────────────────┐\n",
    "                    │   Image Quality Gate             │\n",
    "                    │   (blur, coverage, brightness)   │\n",
    "                    └──────────────┬──────────────────┘\n",
    "                      PASS         │         FAIL → feedback to user\n",
    "                    ┌──────────────▼──────────────────┐\n",
    "                    │   Angle Classifier               │\n",
    "                    │   front / rear / side / interior │\n",
    "                    └──────────────┬──────────────────┘\n",
    "                                   │\n",
    "          ┌────────────────────────┼────────────────────────┐\n",
    "          │                        │                        │\n",
    "    ┌─────▼──────┐          ┌──────▼─────┐          ┌──────▼──────┐\n",
    "    │ Damage Det.│          │ Colour ID  │          │ Odometer OCR│\n",
    "    │ YOLOv8     │          │ K-means /  │          │ TrOCR       │\n",
    "    │            │          │ ViT embed. │          │             │\n",
    "    └─────┬──────┘          └──────┬─────┘          └──────┬──────┘\n",
    "          └────────────────────────┼────────────────────────┘\n",
    "                                   │\n",
    "                    ┌──────────────▼──────────────────┐\n",
    "                    │   Score Aggregation & Rules      │\n",
    "                    │   condition_score = f(damage,    │\n",
    "                    │     age, km_visual, panel_count) │\n",
    "                    └──────────────┬──────────────────┘\n",
    "                                   │\n",
    "                    ┌──────────────▼──────────────────┐\n",
    "                    │   Valuation Adjustment Signal    │\n",
    "                    │   → feeds into Section A model   │\n",
    "                    └─────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluation Strategy\n",
    "\n",
    "### Damage Detection (object detection)\n",
    "- **mAP@0.5** per damage class\n",
    "- **Precision / Recall** at business-relevant operating points  \n",
    "  (prefer high recall for severe damage — false negatives cost more than false positives)\n",
    "\n",
    "### Condition Score (regression)\n",
    "- **MAE / RMSE** vs human assessor ground truth\n",
    "- **Pearson / Spearman correlation** with final adjusted sale price\n",
    "- **Inter-rater agreement** (Cohen's κ on ordinal score buckets)\n",
    "\n",
    "### End-to-End Business Metric\n",
    "- **Δ valuation accuracy**: Does adding the image score reduce MAE in Section A's price model?  \n",
    "  Measure: `MAE_with_image_feature` vs `MAE_without`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Practical Constraints & Trade-offs\n",
    "\n",
    "| Constraint | Consideration |\n",
    "|------------|---------------|\n",
    "| **Data labelling cost** | Start with foundation model (Approach C) to bootstrap labels cheaply; then fine-tune YOLO |\n",
    "| **Inference latency** | YOLOv8-nano gives <50 ms on CPU; adequate for near-real-time assessment |\n",
    "| **Model updates** | Damage styles evolve (EV battery enclosures, new materials); plan quarterly re-training |\n",
    "| **Explainability** | Bounding boxes + severity labels are explainable to assessors and customers |\n",
    "| **Reliability** | Always provide a human override path; model is advisory, not authoritative |\n",
    "| **Scale** | At 10k assessments/day with 5 images each → 50k images/day; batch GPU inference is cost-effective |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. MVP Implementation Roadmap\n",
    "\n",
    "### Phase 1 – Foundation (2–4 weeks)\n",
    "- Image quality gate using OpenCV (Laplacian variance for blur, coverage heuristics)\n",
    "- Zero-shot damage classification with a vision LLM → generate weak labels\n",
    "- Evaluate LLM output quality against 200 human-labelled images\n",
    "\n",
    "### Phase 2 – Supervised Model (4–8 weeks)\n",
    "- Label 2,000–5,000 images (bounding boxes) using Label Studio\n",
    "- Fine-tune YOLOv8-m on damage classes\n",
    "- A/B test: does image-derived condition score improve Section A model's MAE?\n",
    "\n",
    "### Phase 3 – Production Integration (ongoing)\n",
    "- Wrap in FastAPI microservice; integrate with vehicle assessment workflow\n",
    "- Active learning loop: flag uncertain predictions for human review → grow labelled set\n",
    "- Drift monitoring: track prediction distribution shifts over time"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
